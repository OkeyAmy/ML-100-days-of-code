{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe9884bb-1498-4dd9-8457-56b82deff2dc",
   "metadata": {},
   "source": [
    "# What Linear Regression training algorithm can you use if you have a training set with millions of features?\n",
    "\n",
    "Ans: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e1ee0f-3af6-460d-b930-1150715d1314",
   "metadata": {},
   "source": [
    "# Suppose the features in your training set have very different scales. What algorithms might suffer from this, and how? What can you do about it?\n",
    "\n",
    "Ans: Gradient Descent can suffer from it because it will take take much time to converge at the opyimal minumum. We should use the StandardScaler() from sklearn to scale our features input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6642f2f-2239-4254-b032-6d53d16219d7",
   "metadata": {},
   "source": [
    "# Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?\n",
    "\n",
    "Ans: Yes, when our learning rate are either not to high or low can get struck at the local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b814de-1547-4d47-9779-5e046070de55",
   "metadata": {},
   "source": [
    "# Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?\n",
    "\n",
    "Ans: Yes, they do but they are some slight difference it outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1770b18-4c2e-45d3-9845-8dfe5d953526",
   "metadata": {},
   "source": [
    "# Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?\n",
    "\n",
    "Ans: It is probabaly overfitting and the we can fix it is to collect more data or scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b3f7e-2c0b-4f36-9246-82e242da1089",
   "metadata": {},
   "source": [
    "## Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up\n",
    "\n",
    "Ans: No\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f8bb3-7e29-4675-8adc-92dd7c27a4f1",
   "metadata": {},
   "source": [
    "# Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?\n",
    "\n",
    "Ans: Mini-batch Gradient Descent is likey to get to the optimal oslution fastest because it takes small random instances. Reduce the learning rate but don't make it too big or too large of the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000f16b-5dee-464d-96f7-f224f7f569e4",
   "metadata": {},
   "source": [
    "# Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?\n",
    "\n",
    "Ans: It is likely overfitting. We should train it with more data. Regularized the model(like reducing the number of degrees). As stop traing once it reaches the optimal solution known as early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b8038a-943d-41b2-b3e5-1cf9d273d0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
